{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a583375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re, glob, os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a0f7cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML saved to debug_page.html for inspection\n",
      "\n",
      "Found 82 rows in table\n",
      "\n",
      "\n",
      "Total matches found: 80\n",
      "\n",
      "Data saved to 'lol_matches.csv'\n"
     ]
    }
   ],
   "source": [
    "# download match data from lol fandom\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Setup Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = \"https://lol.fandom.com/Special:RunQuery/MatchHistoryGame?MHG%5Bpreload%5D=Tournament&MHG%5Bspl%5D=yes&MHG%5Btournament%5D=2025%20Season%20World%20Championship/Main%20Event&_run=\"\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for table to load and images to render\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "    time.sleep(3)  # Additional wait for dynamic content\n",
    "    \n",
    "    # Get page source\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Debug: Save HTML to file\n",
    "    with open('debug_page.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    print(\"HTML saved to debug_page.html for inspection\\n\")\n",
    "    \n",
    "    # Helper function to extract champion names from various sources\n",
    "    def extract_champions(cell):\n",
    "        champions = []\n",
    "        \n",
    "        # Method 1: Check for CSS sprite elements (spans/divs with champion classes)\n",
    "        for elem in cell.find_all(['span', 'div', 'a']):\n",
    "            # Check class names for champion names\n",
    "            classes = elem.get('class', [])\n",
    "            for cls in classes:\n",
    "                if 'champion' in cls.lower() and 'sprite' not in cls.lower():\n",
    "                    parts = cls.split('-')\n",
    "                    if len(parts) > 1:\n",
    "                        champ_name = parts[-1].replace('_', ' ').title()\n",
    "                        if champ_name and len(champ_name) > 1 and champ_name.lower() != 'sprite':\n",
    "                            champions.append(champ_name)\n",
    "            \n",
    "            # Check title attribute (hover tooltip)\n",
    "            title = elem.get('title', '').strip()\n",
    "            if title and not any(x in title.lower() for x in ['edit', 'create', 'redirect', 'file:', 'image:']):\n",
    "                champions.append(title)\n",
    "            \n",
    "            # Check data attributes\n",
    "            for attr in ['data-champion', 'data-param1', 'data-tooltip-content', 'aria-label', 'data-game-name', 'data-name']:\n",
    "                value = elem.get(attr)\n",
    "                if value and value.strip():\n",
    "                    champions.append(value.strip())\n",
    "        \n",
    "        # Method 2: Check img elements\n",
    "        for img in cell.find_all('img'):\n",
    "            name = img.get('title') or img.get('alt') or img.get('data-image-name')\n",
    "            if name and name.strip():\n",
    "                clean_name = re.sub(r'(Link|Image|Square).*$', '', name.strip(), flags=re.IGNORECASE).strip()\n",
    "                if clean_name and len(clean_name) > 1:\n",
    "                    champions.append(clean_name)\n",
    "        \n",
    "        # Method 3: Check anchor/link elements\n",
    "        for link in cell.find_all('a'):\n",
    "            title = link.get('title', '').strip()\n",
    "            href = link.get('href', '')\n",
    "            \n",
    "            if title and not any(x in title.lower() for x in ['edit', 'create', 'redirect', 'file:', 'image:']):\n",
    "                champions.append(title)\n",
    "            elif '/wiki/' in href and 'File:' not in href and 'Image:' not in href:\n",
    "                champ_name = href.split('/wiki/')[-1].split('#')[0].replace('_', ' ')\n",
    "                if champ_name and len(champ_name) > 1:\n",
    "                    champions.append(champ_name)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_champions = []\n",
    "        for champ in champions:\n",
    "            # Filter out \"Sprite\" and other unwanted values\n",
    "            if champ not in seen and champ and champ.lower() not in ['sprite', 'champion']:\n",
    "                seen.add(champ)\n",
    "                unique_champions.append(champ)\n",
    "        \n",
    "        return unique_champions\n",
    "    \n",
    "    # Helper function to extract team name\n",
    "    def extract_team_name(cell):\n",
    "        # Method 1: Check img title (tooltip on hover)\n",
    "        img = cell.find('img')\n",
    "        if img:\n",
    "            team = img.get('title') or img.get('alt') or img.get('data-image-name')\n",
    "            if team and team.strip():\n",
    "                clean_team = re.sub(r'logo.*$', '', team.strip(), flags=re.IGNORECASE).strip()\n",
    "                return clean_team\n",
    "        \n",
    "        # Method 2: Check link title\n",
    "        link = cell.find('a')\n",
    "        if link:\n",
    "            team = link.get('title', '').strip()\n",
    "            if team and not any(x in team.lower() for x in ['edit', 'create']):\n",
    "                return team\n",
    "            \n",
    "            href = link.get('href', '')\n",
    "            if '/wiki/' in href:\n",
    "                team_name = href.split('/wiki/')[-1].split('#')[0].replace('_', ' ')\n",
    "                if team_name:\n",
    "                    return team_name\n",
    "        \n",
    "        # Method 3: Check data attributes\n",
    "        for elem in cell.find_all(['span', 'div', 'img', 'a']):\n",
    "            for attr in ['data-team', 'data-tooltip-content', 'aria-label']:\n",
    "                value = elem.get(attr)\n",
    "                if value and value.strip():\n",
    "                    return value.strip()\n",
    "        \n",
    "        # Last resort: text content\n",
    "        text = cell.get_text(strip=True)\n",
    "        if text and text not in ['⁠', '⁠⁠', '⁠⁠⁠', '']:\n",
    "            return text\n",
    "        \n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Find the main table\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    # Extract data\n",
    "    matches = []\n",
    "    \n",
    "    rows = table.find_all('tr')[1:]  # Skip header row\n",
    "    \n",
    "    print(f\"Found {len(rows)} rows in table\\n\")\n",
    "    \n",
    "    for idx, row in enumerate(rows):\n",
    "        cells = row.find_all('td')\n",
    "        \n",
    "        if len(cells) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Debug first row\n",
    "        if idx == 0:\n",
    "            print(\"DEBUG - First row structure:\")\n",
    "            print(f\"\\nCell 5 (Blue Bans):\")\n",
    "            print(f\"  Spans: {len(cells[5].find_all('span'))}\")\n",
    "            print(f\"  Divs: {len(cells[5].find_all('div'))}\")\n",
    "            print(f\"  Links: {len(cells[5].find_all('a'))}\")\n",
    "            print(f\"  HTML sample: {str(cells[5])[:800]}\")\n",
    "            \n",
    "            first_span = cells[5].find('span')\n",
    "            if first_span:\n",
    "                print(f\"\\nFirst span element:\")\n",
    "                print(f\"  Classes: {first_span.get('class', [])}\")\n",
    "                print(f\"  All attributes: {first_span.attrs}\")\n",
    "            \n",
    "            first_link = cells[5].find('a')\n",
    "            if first_link:\n",
    "                print(f\"\\nFirst link element:\")\n",
    "                print(f\"  href: {first_link.get('href', '')}\")\n",
    "                print(f\"  title: {first_link.get('title', '')}\")\n",
    "                print(f\"  All attributes: {first_link.attrs}\")\n",
    "            print()\n",
    "        \n",
    "        match_data = {\n",
    "            'Date': cells[0].get_text(strip=True),\n",
    "            'Patch': cells[1].get_text(strip=True),\n",
    "            'Blue_Team': extract_team_name(cells[2]),\n",
    "            'Red_Team': extract_team_name(cells[3]),\n",
    "            'Winner': extract_team_name(cells[4]),\n",
    "            'Blue_Bans': extract_champions(cells[5]),\n",
    "            'Red_Bans': extract_champions(cells[6]),\n",
    "            'Blue_Picks': extract_champions(cells[7]),\n",
    "            'Red_Picks': extract_champions(cells[8]),\n",
    "            'Blue_Roster': cells[9].get_text(strip=True) if len(cells) > 9 else '',\n",
    "            'Red_Roster': cells[10].get_text(strip=True) if len(cells) > 10 else ''\n",
    "        }\n",
    "        \n",
    "        matches.append(match_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(matches)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nTotal matches found: {len(matches)}\\n\")\n",
    "\n",
    "    # group matches by series, some are best of 1, some best of 3 or 5. Continuous matches with same teams (Blue or Red) are in same series\n",
    "    df['Series_ID'] = (((df['Blue_Team'] != df['Blue_Team'].shift()) & (df['Blue_Team'] != df['Red_Team'].shift()))\n",
    "                       |((df['Blue_Team'] != df['Red_Team'].shift()) & (df['Red_Team'] != df['Red_Team'].shift()))).cumsum()\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('lol_matches.csv', index=False)\n",
    "    print(\"Data saved to 'lol_matches.csv'\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76391a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Data saved to 'lol_matches_expanded.csv'\n"
     ]
    }
   ],
   "source": [
    "# split the bans, picks, roster into separate columns\n",
    "df_expanded = df.copy()\n",
    "\n",
    "def expand_ban_lists(row, col_name, prefix):\n",
    "    champ_list = row[col_name]\n",
    "    for i in range(5):  # Assuming max 5 bans/picks\n",
    "        champ = champ_list[i] if i < len(champ_list) else None\n",
    "        row[f\"{prefix}{i+1}\"] = champ\n",
    "    return row\n",
    "\n",
    "def expand_champ_lists(row, col_name, prefix):\n",
    "    champ_list = row[col_name]\n",
    "    for i in range(5):  # Assuming max 5 bans/picks\n",
    "        champ = champ_list[i] if i < len(champ_list) else None\n",
    "        roles = ['Top', 'Jungle', 'Mid', 'ADC', 'Support']\n",
    "        row[f\"{prefix}{roles[i]}\"] = champ\n",
    "    return row\n",
    "\n",
    "def expand_roster(row, col_name, prefix):\n",
    "    roster = [name.strip() for name in row[col_name].split(',')] if row[col_name] else []\n",
    "    for i in range(5):  # Assuming max 5 players\n",
    "        player = roster[i] if i < len(roster) else None\n",
    "        roles = ['Top', 'Jungle', 'Mid', 'ADC', 'Support']\n",
    "        row[f\"{prefix}{roles[i]}\"] = player\n",
    "    return row\n",
    "\n",
    "# expand picks and bans lists\n",
    "df_expanded = df_expanded.apply(expand_ban_lists, axis=1, col_name='Blue_Bans', prefix='Blue_Ban_')\n",
    "df_expanded = df_expanded.apply(expand_ban_lists, axis=1, col_name='Red_Bans', prefix='Red_Ban_')\n",
    "df_expanded = df_expanded.apply(expand_champ_lists, axis=1, col_name='Blue_Picks', prefix='Blue_Pick_')\n",
    "df_expanded = df_expanded.apply(expand_champ_lists, axis=1, col_name='Red_Picks', prefix='Red_Pick_')\n",
    "\n",
    "# expand roster lists\n",
    "df_expanded = df_expanded.apply(expand_roster, axis=1, col_name='Blue_Roster', prefix='Blue_Player_')\n",
    "df_expanded = df_expanded.apply(expand_roster, axis=1, col_name='Red_Roster', prefix='Red_Player_')\n",
    "\n",
    "df_expanded.drop(columns=['Patch', 'Blue_Bans', 'Red_Bans', 'Blue_Picks', 'Red_Picks', 'Blue_Roster', 'Red_Roster'], inplace=True)\n",
    "\n",
    "df_expanded.to_csv('lol_matches_expanded.csv', index=False)\n",
    "print(\"Expanded Data saved to 'lol_matches_expanded.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
